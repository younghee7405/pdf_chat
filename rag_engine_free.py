"""
RAG ì—”ì§„ - ë¬´ë£Œ ë²„ì „ (HuggingFace ì„ë² ë”© ì‚¬ìš©)
OpenAI API í¬ë ˆë”§ì´ ì—†ì„ ë•Œ ì‚¬ìš©
"""
import os
import pickle
from typing import List, Dict
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from openai import OpenAI


class RAGEngineFree:
    """RAG íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤ (ë¬´ë£Œ ì„ë² ë”© ì‚¬ìš©)"""
    
    def __init__(self, openai_api_key: str):
        """
        Args:
            openai_api_key: OpenAI API í‚¤ (ë‹µë³€ ìƒì„±ìš©ë§Œ ì‚¬ìš©)
        """
        self.api_key = openai_api_key
        
        # HuggingFace ì„ë² ë”© ì‚¬ìš© (ë¬´ë£Œ, ë¡œì»¬)
        print("ğŸ“¥ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ë‹¤ìš´ë¡œë“œ)")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        self.vector_store = None
        self.chunks_metadata = []
        self.client = OpenAI(api_key=openai_api_key)
        
    def build_vector_store(self, chunks: List[Dict]) -> None:
        """
        ì²­í¬ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
        
        Args:
            chunks: PDF ì²˜ë¦¬ë¡œë¶€í„° ì–»ì€ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ì²­í¬ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.chunks_metadata = chunks
        
        # LangChain Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for chunk in chunks:
            doc = Document(
                page_content=chunk['text'],
                metadata={
                    'chunk_id': chunk['chunk_id'],
                    'page_number': chunk['page_number'],
                    'source': chunk['source']
                }
            )
            documents.append(doc)
        
        # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"ğŸ“Š {len(documents)}ê°œì˜ ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ì¤‘... (ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©)")
        self.vector_store = FAISS.from_documents(documents, self.embeddings)
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")
        
    def save_vector_store(self, path: str = "vector_store_free") -> None:
        """
        ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
        
        Args:
            path: ì €ì¥ ê²½ë¡œ
        """
        if self.vector_store is None:
            raise ValueError("ì €ì¥í•  ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        os.makedirs(path, exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        self.vector_store.save_local(path)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = os.path.join(path, "chunks_metadata.pkl")
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.chunks_metadata, f)
        
        print(f"ğŸ’¾ ë²¡í„° ìŠ¤í† ì–´ê°€ {path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def load_vector_store(self, path: str = "vector_store_free") -> None:
        """
        ë””ìŠ¤í¬ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        
        Args:
            path: ë¡œë“œ ê²½ë¡œ
        """
        if not os.path.exists(path):
            raise ValueError(f"{path} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.vector_store = FAISS.load_local(
            path, 
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = os.path.join(path, "chunks_metadata.pkl")
        if os.path.exists(metadata_path):
            with open(metadata_path, 'rb') as f:
                self.chunks_metadata = pickle.load(f)
        
        print(f"ğŸ“‚ ë²¡í„° ìŠ¤í† ì–´ê°€ {path}ë¡œë¶€í„° ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def search(self, query: str, k: int = 3) -> List[Dict]:
        """
        ì§ˆì˜ì— ëŒ€í•œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            List[Dict]: ê²€ìƒ‰ëœ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„°
        """
        if self.vector_store is None:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
        results = self.vector_store.similarity_search_with_score(query, k=k)
        
        search_results = []
        for doc, score in results:
            result = {
                'text': doc.page_content,
                'page_number': doc.metadata['page_number'],
                'chunk_id': doc.metadata['chunk_id'],
                'source': doc.metadata['source'],
                'similarity_score': float(score)
            }
            search_results.append(result)
        
        return search_results
    
    def generate_answer(self, query: str, search_results: List[Dict], system_prompt: str = None) -> Dict:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            search_results: ê²€ìƒ‰ëœ ì²­í¬ë“¤
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            
        Returns:
            Dict: ë‹µë³€ ë° ì°¸ì¡° í˜ì´ì§€ ì •ë³´
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        page_numbers = set()
        
        for i, result in enumerate(search_results, 1):
            context_parts.append(
                f"[ë¬¸ì„œ {i} - í˜ì´ì§€ {result['page_number']}]\n{result['text']}\n"
            )
            page_numbers.add(result['page_number'])
        
        context = "\n".join(context_parts)
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if system_prompt is None:
            system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì„¤ì¹˜ ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ë‹¨ê³„ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
1. **ê°œìš”**: ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½
2. **ë‹¨ê³„ë³„ ì„¤ëª…**: êµ¬ì²´ì ì¸ ë‹¨ê³„ë¥¼ ë²ˆí˜¸ë¡œ ë‚˜ì—´
3. **ì°¸ê³ ì‚¬í•­**: ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        
        # OpenAI API í˜¸ì¶œ
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}"""}
        ]
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7,
            max_tokens=1500
        )
        
        answer = response.choices[0].message.content
        
        return {
            'answer': answer,
            'referenced_pages': sorted(list(page_numbers)),
            'source_chunks': search_results,
            'model': response.model,
            'total_tokens': response.usage.total_tokens
        }
    
    def query(self, question: str, k: int = 3, system_prompt: str = None) -> Dict:
        """
        ì§ˆì˜ì— ëŒ€í•œ ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜
            system_prompt: ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            Dict: ë‹µë³€, ì°¸ì¡° í˜ì´ì§€, ê²€ìƒ‰ ê²°ê³¼ ë“±
        """
        # 1. ê²€ìƒ‰
        search_results = self.search(question, k=k)
        
        # 2. ë‹µë³€ ìƒì„±
        result = self.generate_answer(question, search_results, system_prompt)
        
        # 3. ì§ˆë¬¸ ì¶”ê°€
        result['question'] = question
        
        return result

